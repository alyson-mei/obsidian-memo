### TF- IDF
Term frequency  $$TF = f_{t,d} = \textrm{freq}_{term, tocument}$$f
Inverse document frequency
$$IDF = log(1/f_{t, corpus})  $$
$$TF \cdot IDF$$
### PCA
Слово по контексту
(e.g. контекст - набор 10 слов (5 слева, 5 справа); если два слова оказались в нём, то count += 1)

Матрица $W \times W$ (симметричная), на $(v, w)$ частота встречи $v$ и $w$ в одном контексте

Использование PCA  для понижения размерности (получаем вектора эмбеддингов)

### word2vec
Хотим: похожие слова <-> похожие векторы

n-gram - последовательность n токенов
skip-gram - последовательность токенов, в котором выколот токен (обычно центральный) 

example: ... problems turning **into** banking crises as ... 
$p(w_{t-2}|w_t)$, $p(w_{t-1}|w_t)$, $p(w_{t+1}|w_t)$, $p(w_{t+2}|w_t)$


